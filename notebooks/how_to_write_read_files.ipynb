{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何使用Python讀寫檔案資料？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Astropy\n",
    "  * <a href=\"http://docs.astropy.org/en/stable/io/unified.html\" target=\"_blank\">Unified file read/write interface</a>\n",
    "  * <a href=\"http://docs.astropy.org/en/stable/io/fits/index.html\" target=\"_blank\">FITS File handling (astropy.io.fits)</a>\n",
    "  * <a href=\"http://docs.astropy.org/en/stable/io/ascii/index.html\" target=\"_blank\">ASCII Tables (astropy.io.ascii)</a>\n",
    "  * <a href=\"http://docs.astropy.org/en/stable/io/votable/index.html\" target=\"_blank\">VOTable XML handling (astropy.io.votable)</a>\n",
    "  * <a href=\"http://docs.astropy.org/en/stable/io/misc.html\" target=\"_blank\">Miscellaneous Input/Output (astropy.io.misc)</a>\n",
    "  * <a href=\"http://docs.astropy.org/en/stable/vo/index.html\" target=\"_blank\">Virtual Observatory Access (astropy.vo)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備工作：所需檔案下載\n",
    "注意：python2 與 python3 下的 urllib有點不同, 下面修改成 python2 的環境, python3的語法會 comment 起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面兩個 cell 分別演示如何使用 urllib來透過 url 抓取資料\n",
    "*  SDSS的 fits binary table, 裡面存的是光譜\n",
    "*  NOAA的太陽黑子預測, 純文字檔案\n",
    "\n",
    "執行後會多出兩個檔案 \"spec.fits\"和 \"astro_table.txt\", 可以用以前熟悉的工具先看看資料的內容與格式, 這樣比較有 fu..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spec.fits', <httplib.HTTPMessage instance at 0x103fac440>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitsurl=\"http://dr10.sdss3.org/sas/dr10/sdss/spectro/redux/26/spectra/0651/spec-0651-52141-0569.fits\"\n",
    "#urllib.request.urlretrieve(fitsurl,\"spec.fits\")\n",
    "urllib.urlretrieve(fitsurl,\"spec.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('astro_table.txt', <httplib.HTTPMessage instance at 0x103fc0d88>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texturl=\"http://services.swpc.noaa.gov/text/predicted-sunspot-radio-flux.txt\"\n",
    "#urllib.request.urlretrieve(texturl, \"astro_table.txt\")\n",
    "urllib.urlretrieve(texturl, \"astro_table.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab：請修改 url 為你自己感興趣的資料來源, 下載資料並存在自己命名的檔案中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 範例：用Astropy 讀 FITS檔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 參考文件: <a href=\"http://docs.astropy.org/en/stable/io/fits/index.html\" target=\"_blank\">Astropy FITS File handling</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: spec.fits\n",
      "No.    Name         Type      Cards   Dimensions   Format\n",
      "0    PRIMARY     PrimaryHDU     141   ()              \n",
      "1    COADD       BinTableHDU     26   3829R x 8C   [E, E, E, J, J, E, E, E]   \n",
      "2    SPECOBJ     BinTableHDU    262   1R x 126C    [6A, 4A, 16A, 23A, 16A, 8A, E, E, E, J, E, E, J, B, B, B, B, B, B, J, 22A, 19A, 19A, 22A, 19A, I, 3A, 3A, 1A, J, D, D, D, E, E, 19A, 8A, J, J, J, J, K, K, J, J, J, J, J, J, K, K, K, K, I, J, J, J, J, 5J, D, D, 6A, 21A, E, E, E, J, E, 24A, 10J, J, 10E, E, E, E, E, E, E, J, E, E, E, J, E, 5E, E, 10E, 10E, 10E, 5E, 5E, 5E, 5E, 5E, J, J, E, E, E, E, E, E, 25A, 21A, 10A, E, E, E, E, E, E, E, E, J, E, E, J, 1A, 1A, E, E, J, J, 1A, 5E, 5E]   \n",
      "3    SPZLINE     BinTableHDU     48   29R x 19C    [J, J, J, 13A, D, E, E, E, E, E, E, E, E, E, E, J, J, E, E]   \n",
      "4    B2-00010589-00010594-00010595  BinTableHDU    146   2047R x 7C   [E, E, E, J, E, E, E]   \n",
      "5    B2-00010590-00010594-00010595  BinTableHDU    146   2047R x 7C   [E, E, E, J, E, E, E]   \n",
      "6    B2-00010591-00010594-00010595  BinTableHDU    146   2047R x 7C   [E, E, E, J, E, E, E]   \n",
      "7    B2-00010592-00010594-00010595  BinTableHDU    146   2047R x 7C   [E, E, E, J, E, E, E]   \n",
      "8    R2-00010589-00010594-00010595  BinTableHDU    146   2044R x 7C   [E, E, E, J, E, E, E]   \n",
      "9    R2-00010590-00010594-00010595  BinTableHDU    146   2044R x 7C   [E, E, E, J, E, E, E]   \n",
      "10   R2-00010591-00010594-00010595  BinTableHDU    146   2044R x 7C   [E, E, E, J, E, E, E]   \n",
      "11   R2-00010592-00010594-00010595  BinTableHDU    146   2044R x 7C   [E, E, E, J, E, E, E]   \n"
     ]
    }
   ],
   "source": [
    "from astropy.io import fits\n",
    "hdulist = fits.open('spec.fits')\n",
    "hdulist.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdulist 顧名思義, 就是 a list of Header/Data Unit (廢話), 關於 FITS檔案的格式, 請參閱 http://fits.gsfc.nasa.gov/fits_primer.html 裡面有詳盡的說明\n",
    "\n",
    "hdulist.info() 會把每個 element的摘要列出來, 以上面的例子來說, 就是讀進來的 spec.fits 他有 12個 units, 第 0個就是個純的 header, 第 1,2,3個 unit應該是一些 meta data, 真正的科學資料是從第 4個 unit開始, 格式為 binary table, 接下來我們就用第四個 unit 來作演示\n",
    "\n",
    "通常在不太了解新的物件時, dir() 是我們的好朋友, 方便我們猜測可以對這個陌生的物件做些什麼事情。 所以, 就 dir()吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_EXCLUDE',\n",
       " '_MASK',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_buffer',\n",
       " '_calculate_checksum',\n",
       " '_calculate_datasum',\n",
       " '_calculate_datasum_with_heap',\n",
       " '_char_encode',\n",
       " '_clear_table_keywords',\n",
       " '_close',\n",
       " '_columns_type',\n",
       " '_compute_checksum',\n",
       " '_compute_hdu_checksum',\n",
       " '_datLoc',\n",
       " '_datSpan',\n",
       " '_data_loaded',\n",
       " '_data_needs_rescale',\n",
       " '_data_offset',\n",
       " '_data_replaced',\n",
       " '_data_size',\n",
       " '_data_type',\n",
       " '_default_name',\n",
       " '_dump_coldefs',\n",
       " '_dump_data',\n",
       " '_encode_byte',\n",
       " '_ext_comment',\n",
       " '_extension',\n",
       " '_file',\n",
       " '_get_raw_data',\n",
       " '_get_tbdata',\n",
       " '_get_timestamp',\n",
       " '_has_data',\n",
       " '_hdrLoc',\n",
       " '_hdu_registry',\n",
       " '_header',\n",
       " '_header_offset',\n",
       " '_init_tbdata',\n",
       " '_load_coldefs',\n",
       " '_load_data',\n",
       " '_manages_own_heap',\n",
       " '_new',\n",
       " '_nrows',\n",
       " '_output_checksum',\n",
       " '_padding_byte',\n",
       " '_populate_table_keywords',\n",
       " '_postwriteto',\n",
       " '_prewriteto',\n",
       " '_readfrom_internal',\n",
       " '_standard',\n",
       " '_summary',\n",
       " '_tdump_file_format',\n",
       " '_theap',\n",
       " '_uint',\n",
       " '_update_checksum',\n",
       " '_update_column_added',\n",
       " '_update_column_attribute_changed',\n",
       " '_update_column_removed',\n",
       " '_update_uint_scale_keywords',\n",
       " '_verify',\n",
       " '_verify_checksum_datasum',\n",
       " '_writedata',\n",
       " '_writedata_by_row',\n",
       " '_writedata_direct_copy',\n",
       " '_writedata_internal',\n",
       " '_writeheader',\n",
       " '_writeto',\n",
       " 'add_checksum',\n",
       " 'add_datasum',\n",
       " 'columns',\n",
       " 'copy',\n",
       " 'data',\n",
       " 'dump',\n",
       " 'filebytes',\n",
       " 'fileinfo',\n",
       " 'from_columns',\n",
       " 'fromstring',\n",
       " 'header',\n",
       " 'is_image',\n",
       " 'level',\n",
       " 'load',\n",
       " 'match_header',\n",
       " 'name',\n",
       " 'readfrom',\n",
       " 'register_hdu',\n",
       " 'req_cards',\n",
       " 'run_option',\n",
       " 'size',\n",
       " 'tcreate',\n",
       " 'tdump',\n",
       " 'unregister_hdu',\n",
       " 'update',\n",
       " 'update_ext_name',\n",
       " 'update_ext_version',\n",
       " 'ver',\n",
       " 'verify',\n",
       " 'verify_checksum',\n",
       " 'verify_datasum',\n",
       " 'writeto']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hdulist[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雖然落落長一串, 希望你的眼睛沒有什麼業障, 可以發現有一些有趣的屬性/方法, 像是 data, dump, writeto... 等等\n",
    "接下來就繼續冒險前進, 用點點 (.) 來探索新世界吧\n",
    "\n",
    "所以就 .data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FITS_rec([(17.107233, 3.790184, 0.00034346466, 0, 0.77565843, 16.30872, 0.42700836),\n",
       "       (23.673752, 3.7900956, 0.00058561546, 0, 0.77561331, 17.235472, 0.41987428),\n",
       "       (28.730856, 3.7900071, 0.00084290426, 0, 0.77556837, 17.819456, 0.41240782),\n",
       "       ...,\n",
       "       (31.470488, 3.5814037, 0.012394244, 0, 1.0769527, 12.84608, 0.59983635),\n",
       "       (30.73415, 3.5812922, 0.011396203, 0, 1.0772977, 12.612031, 0.60809124),\n",
       "       (39.182308, 3.5811803, 0.0095504085, 0, 1.0776428, 12.544607, 0.62002754)], \n",
       "      dtype=(numpy.record, [('flux', '>f4'), ('loglam', '>f4'), ('ivar', '>f4'), ('mask', '>i4'), ('wdisp', '>f4'), ('sky', '>f4'), ('calib', '>f4')]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdulist[4].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 你看看你, data這不就來了嗎~~~ \n",
    "\n",
    "好, 看來這像是個 numpy 的 record array (or structured array), 格式長這樣 : ([(v1, v2, v3...), (), .... ()], dtype=(type1, type2, type3...)), 不難猜測 v1 對應 type1... 以此類推\n",
    "\n",
    "* 所以, 17.107是 flux, 3.79是 loglam.... 等等等等\n",
    "\n",
    "我想 loglam是 lambda 的 log這種事應該猜的到齁？還猜不到的話, 恭喜你, 還中毒未深~~~ \n",
    "要抓出 flux 或任意 type的方法也很直覺"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17.10723305,  23.67375183,  28.73085594, ...,  31.47048759,\n",
       "        30.73414993,  39.1823082 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdulist[4].data[\"flux\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好滴... 這個演示就先告個段落, 記得要把打開的東西 (hdulist) 關起來, 養成好習慣以後才不會很難改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdulist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 範例：用Numpy讀寫檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 尚需要更合適的檔案來當範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "讀進的array = np.loadtxt(\"檔名\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 對讀進的array做一些運算後存成新array，然後將新array寫到一個新檔案中 (代補)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('新檔名', 新array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 範例：用Astropy讀寫 ASCII 檔案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####### 尚需要更合適的檔案來當範例\n",
    "####### 更新使用 NOAA的太陽黑子預報 http://services.swpc.noaa.gov/text/predicted-sunspot-radio-flux.txt\n",
    "\n",
    "astropy 的 ascii 提供相當完整的 ascii 讀取方案, 除了基本的格式外, 還支援 csv, cds (vizier), latex, DAOPHOT, Sextractor... 天文領域中常見的 ascii 表格大概都可以無痛讀取, 不需要自己花時間解析\n",
    "\n",
    "用法也很直覺, 如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1 col2 col3 col4 col5  col6  col7 col8\n",
      "---- ---- ---- ---- ---- ----- ----- ----\n",
      "2016    1 33.7 34.7 32.7 100.3 101.3 99.3\n",
      "2016    2 33.9 35.9 31.9  99.5 100.5 98.5\n",
      "2016    3 33.6 36.6 30.6  98.7 100.7 96.7\n",
      "2016    4 33.2 38.2 28.2  97.9 100.9 94.9\n",
      "2016    5 33.1 38.1 28.1  96.7 100.7 92.7\n",
      "2016    6 33.0 39.0 27.0  95.0  99.0 91.0\n",
      "2016    7 33.0 40.0 26.0  93.5  98.5 88.5\n",
      "2016    8 32.8 39.8 25.8  92.2  98.2 86.2\n",
      "2016    9 32.6 40.6 24.6  91.3  98.3 84.3\n",
      "2016   10 32.7 41.7 23.7  90.7  98.7 82.7\n",
      " ...  ...  ...  ...  ...   ...   ...  ...\n",
      "2019    2  8.3 18.3  0.0  66.3  75.3 60.0\n",
      "2019    3  7.8 17.8  0.0  65.9  74.9 60.0\n",
      "2019    4  7.3 17.3  0.0  65.4  74.4 60.0\n",
      "2019    5  6.8 16.8  0.0  65.0  74.0 60.0\n",
      "2019    6  6.4 16.4  0.0  64.5  73.5 60.0\n",
      "2019    7  5.9 15.9  0.0  64.1  73.1 60.0\n",
      "2019    8  5.5 15.5  0.0  63.8  72.8 60.0\n",
      "2019    9  5.1 15.1  0.0  63.4  72.4 60.0\n",
      "2019   10  4.8 14.8  0.0  63.1  72.1 60.0\n",
      "2019   11  4.4 14.4  0.0  62.8  71.8 60.0\n",
      "2019   12  4.1 14.1  0.0  62.5  71.5 60.0\n",
      "Length = 48 rows\n"
     ]
    }
   ],
   "source": [
    "from astropy.io import ascii\n",
    "#data = ascii.read(\"astro_table.txt\")\n",
    "data = ascii.read(\"astro_table.txt\",data_start=2)  \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read() 出來的是個 table 物件, dir()的樂趣我就留給各位, 下面直接舉兩個常用的例子:\n",
    "* 抽取特定的 column(s)\n",
    "* 使用 where() 來抓取符合條件的 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col5 col1\n",
      "---- ----\n",
      "32.7 2016\n",
      "31.9 2016\n",
      "30.6 2016\n",
      "28.2 2016\n",
      "28.1 2016\n",
      "27.0 2016\n",
      "26.0 2016\n",
      "25.8 2016\n",
      "24.6 2016\n",
      "23.7 2016\n",
      " ...  ...\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      " 0.0 2019\n",
      "Length = 48 rows\n"
     ]
    }
   ],
   "source": [
    "print(data[\"col5\",\"col1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([17, 18, 19, 20, 29, 30, 31, 32, 41, 42, 43, 44]),)\n",
      "col1 col2 col3\n",
      "---- ---- ----\n",
      "2017    6 26.1\n",
      "2017    7 24.9\n",
      "2017    8 23.7\n",
      "2017    9 22.5\n",
      "2018    6 13.7\n",
      "2018    7 12.9\n",
      "2018    8 12.2\n",
      "2018    9 11.5\n",
      "2019    6  6.4\n",
      "2019    7  5.9\n",
      "2019    8  5.5\n",
      "2019    9  5.1\n"
     ]
    }
   ],
   "source": [
    "d = np.where((data[\"col1\"] >= 2017) & (data[\"col1\"] <= 2019) & (data[\"col2\"] >=6) & (data[\"col2\"] <=9))\n",
    "print(d)\n",
    "print(data[d][\"col1\",\"col2\",\"col3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.write(\"out\", format=\"ascii\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
